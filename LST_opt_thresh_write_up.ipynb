{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Determination of Optimal Initial Threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. INTRODUCTION\n",
    "            \n",
    "Using lesion probability maps created from LST’s Lesion Growth Algorithm (LGA) helps to automate the process of mapping out the lesions visible in our MS subjects’ MRI Images. Unfortunately these lesion probability maps require time-consuming manual edits to correct the False Positives and False Negative. To minimize this issue we created a script to run LST multiple times while iterating through all possible parameters, bin threshold and initial threshold, and created a new metric to evaluate which parameters create the most accurate lesion probability map for any given FLAIR-T1 image pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. METHODS\n",
    " \n",
    "### A. \n",
    "Our first step in this experiment required writing a script to run LST’s provided tool for determining the optimum threshold on a given lesion probability map, while iterating through its one parameter, binary threshold while keeping initial threshold the same. (0.3) The scripts are displayed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nipype.interfaces.spm.base import SPMCommand, SPMCommandInputSpec\n",
    "from nipype.interfaces.base import (BaseInterface, TraitedSpec, traits, File,\n",
    "                                    OutputMultiPath, BaseInterfaceInputSpec,\n",
    "                                    isdefined, InputMultiPath)\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.io as nio\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "import os\n",
    "#from utils import doit_workflow\n",
    "\n",
    "\n",
    "class DoitInputSpec(SPMCommandInputSpec):\n",
    "    data_ref = traits.List(File(exists=True), field=\"doit.data_ref\")\n",
    "    bin_thresh = traits.Any(0.5, field=\"doit.bin_thresh\", usedefault=True) #default is 0.5\n",
    "\n",
    "\n",
    "class DoitOutputSpec(TraitedSpec):\n",
    "    csv_file = OutputMultiPath(File(exists=True))\n",
    "\n",
    "\n",
    "class Doit(SPMCommand):\n",
    "    input_spec = DoitInputSpec\n",
    "    output_spec = DoitOutputSpec\n",
    "    _jobtype = 'tools'\n",
    "    _jobname = 'LST'\n",
    "\n",
    "    def _make_matlab_command(self, contents, postscript=None):\n",
    "        len_ref = len(self.inputs.data_ref)\n",
    "\n",
    "        contents = \"\"\"\n",
    "        %% Generated by nipype.interfaces.spm\n",
    "        if isempty(which('spm')),\n",
    "             throw(MException('SPMCheck:NotFound', 'SPM not in matlab path'));\n",
    "        end\n",
    "        [name, version] = spm('ver');\n",
    "        fprintf('SPM version: %s Release: %s',name, version);\n",
    "        fprintf('SPM path: %s', which('spm'));\n",
    "        spm('Defaults','fMRI');\n",
    "\n",
    "        if strcmp(name, 'SPM8') || strcmp(name(1:5), 'SPM12'),\n",
    "           spm_jobman('initcfg');\n",
    "           spm_get_defaults('cmdline', 1);\n",
    "        end\n",
    "        \"\"\"\n",
    "\n",
    "        contents += \"\"\"\n",
    "        jobs{1}.spm.tools.LST.doit.bin_thresh = %f;\n",
    "        \"\"\" % self.inputs.bin_thresh\n",
    "\n",
    "        for i, ref in enumerate(self.inputs.data_ref):\n",
    "            contents += \"\"\"\n",
    "            jobs{1}.spm.tools.LST.doit.data_ref{%d, 1} = '%s';\n",
    "        \"\"\" % (i+1,\n",
    "               ref)\n",
    "\n",
    "        contents += \"\"\"\n",
    "                    spm_jobman('run', jobs);\n",
    "                    \"\"\"\n",
    "        return contents\n",
    "\n",
    "    def _format_arg(self, opt, spec, val):\n",
    "        \"\"\"Convert input to appropriate format for spm\n",
    "        \"\"\"\n",
    "        # import numpy as np\n",
    "        # from nipype.utils.filemanip import copyfiles\n",
    "\n",
    "        # if opt in ['t1_files', 'flair_files']:\n",
    "        #    val2 = copyfiles(val, os.path.abspath(\".\"))\n",
    "        #    return np.array(val2, dtype=object)\n",
    "        print(\"_format_arg opt is: \", opt)\n",
    "        print(\"_format_arg opt is: \", spec)\n",
    "        print(\"_format_arg opt is: \", val)\n",
    "\n",
    "        # TODO change the format of data_ref\n",
    "\n",
    "        return super(Doit, self)._format_arg(opt, spec, val)\n",
    "\n",
    "\n",
    "    def _list_outputs(self):\n",
    "        #from nipype.utils.filemanip import fname_presuffix\n",
    "        from glob import glob\n",
    "        from os.path import join\n",
    "        outputs = self._outputs().get()\n",
    "        print(\"Listing outputs!\")\n",
    "        print(os.path.abspath('.'))\n",
    "\n",
    "        outputs[\"csv_file\"] = glob(join(os.path.abspath('.'), \"LST_doit_*.csv\"))\n",
    "        print(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Three python files were written in the same directory. The script above wraps SPM command into python to run LGA. The file name is doit_spm.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sf713420'\n",
    "\n",
    "import nipype.pipeline.engine as pe\n",
    "import nipype.interfaces.io as nio\n",
    "from nipype.interfaces.utility import IdentityInterface\n",
    "import os\n",
    "import sys\n",
    "print(__file__)\n",
    "print(sys.path)\n",
    "sys.path.append(os.path.dirname(__file__))\n",
    "import numpy as np\n",
    "from doit_spm import Doit\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def doit_workflow(data_ref, bin_thresh, base_dir = None, sink_dir = None):\n",
    "    # data_ref should be a list of pathname\n",
    "    # base_dir is the working directory\n",
    "    # sink_dir is where the data is sinking\n",
    "    if base_dir is None:\n",
    "        base_dir = os.getcwd()\n",
    "        print(\"base_dir is: \", base_dir)\n",
    "    if sink_dir is None:\n",
    "        sink_dir = base_dir\n",
    "        print(\"sink_dir is: \", sink_dir)\n",
    "\n",
    "    count = 0\n",
    "\n",
    "\n",
    "    # inputs\n",
    "    inputspec = pe.Node(IdentityInterface(fields = ['data_ref', 'thresh']), name = 'inputspec')\n",
    "    inputspec.inputs.mandatory_inputs = True\n",
    "    inputspec.inputs.data_ref = data_ref\n",
    "    inputspec.inputs.thresh = bin_thresh\n",
    "\n",
    "\n",
    "    # doit_node\n",
    "    doit_node = pe.Node(name = 'doit_node',\n",
    "                           interface = Doit(),\n",
    "                           #iterfield = ['data_ref']\n",
    "                           )\n",
    "    doit_node.iterables = (\"bin_thresh\", bin_thresh)\n",
    "    # TODO\n",
    "    print(doit_node.iterables)\n",
    "    print(doit_node.inputs.bin_thresh)\n",
    "\n",
    "    #datasink\n",
    "    data_sink = pe.Node(nio.DataSink(), name = 'sinker')\n",
    "    data_sink.inputs.base_directory = sink_dir\n",
    "    data_sink.inputs.container = '.'\n",
    "\n",
    "    # Pipeline assembly\n",
    "    pipeline = pe.Workflow(name = 'pipeline_doit')\n",
    "    pipeline.base_dir = base_dir\n",
    "\n",
    "    pipeline.connect(inputspec, 'data_ref', doit_node, 'data_ref')\n",
    "    pipeline.connect(inputspec, 'thresh', doit_node, 'bin_thresh')\n",
    "    pipeline.connect(doit_node, 'csv_file', data_sink, '@LST_doit')\n",
    "\n",
    "    pipeline.write_graph(graph2use = 'orig')\n",
    "    pipeline.config['Execution'] = {'keep_inputs': True, 'remove_unnecessary_outputs': False}\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A nipype workflow was made to iterate through binary threshold with increment 0.05. The file name is utils.py__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sf713420'\n",
    "\n",
    "import os\n",
    "from nipype.interfaces.base import isdefined\n",
    "from utils import doit_workflow, flatten\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Reading file from the path: \", sys.argv[1], \"\\n\")\n",
    "    data_ref = glob(sys.argv[1])\n",
    "\n",
    "    FLAIR_T1_name = data_ref[0].split('/')[5]\n",
    "    output_dir = '/data/henry1/tristan/LST/opt_thresh_results'\n",
    "    sk_dir = os.path.join(output_dir,\n",
    "                          FLAIR_T1_name\n",
    "                          )\n",
    "\n",
    "    dwf = doit_workflow(data_ref, thresh_array, sink_dir=sk_dir)\n",
    "    dwf.run()\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(os.path.join(sk_dir, '_bin_thresh_' + thresh_array[i]), newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "\n",
    "    csvfile.close()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This script takes the first positional argument as filename for running the LGA DC calculation workflow. The script is name as doit_arg.py__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The csv files are generated for each binary threshold. All of them includes 102 mses which are FLAIR-MPRAGE image pairs. The average Dice Coefficient (DC), Sensitivity (SE) and Specificity (SP) among these mses were calculated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. \n",
    "The next step a script that would run LST’s LGA on a given FLAIR-T1 image pair while iterating through its one parameter, initial threshold was run. The multiple lesion probability maps created would then be analyzed using LST’s provided tool for determining the optimum threshold to evaluate which initial threshold will create a map that requires the least amount of manual edits. The LGA iteration for initial threshold (kappa) increment of 0.05 was run in PBR. 5 test mses (mse3727, mse4413, mse4482, mse4739, mse4754) were run and outputs were calculated using previous code with binary threshold set to 0.3. The code is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sf713420'\n",
    "from nipype.interfaces.base import (BaseInterface, TraitedSpec, traits, File,\n",
    "                                    OutputMultiPath, BaseInterfaceInputSpec,\n",
    "                                    isdefined, InputMultiPath)\n",
    "\n",
    "from ...config import config\n",
    "from glob import glob\n",
    "import os\n",
    "from ...base import register_workflow, PBRBaseInputSpec, PBRBaseInterface\n",
    "from nipype.interfaces.spm.base import SPMCommand, SPMCommandInputSpec\n",
    "\n",
    "class LGAInputSpec(SPMCommandInputSpec):\n",
    "    t1_files = traits.List(File(exists=True), field=\"lga.data_T1\")\n",
    "    flair_files = traits.List(File(exists=True), field=\"lga.data_F2\")\n",
    "    kappa = traits.Float(0.3, field=\"lga.opts_lga.initial\", usedefault=True)\n",
    "    maxiter = traits.Int(50, field=\"lga.opts_lga.maxiter\", usedefault=True)\n",
    "    phi = traits.Float(1.0, field=\"lga.opts_lga.mrf\", usedefault=True)\n",
    "    html_report = traits.Bool(0, field=\"lga.html_report\", usedefault=True)\n",
    "\n",
    "class LGAOutputSpec(TraitedSpec):\n",
    "    lesion_probability_map = OutputMultiPath(File(exists=True))\n",
    "    mat_file = OutputMultiPath(File(exists=True))\n",
    "    bias_corrected_flair = OutputMultiPath(File(exists=True))\n",
    "\n",
    "\n",
    "class LGA(SPMCommand):\n",
    "    input_spec = LGAInputSpec\n",
    "    output_spec = LGAOutputSpec\n",
    "    _jobtype = 'tools'\n",
    "    _jobname = 'LST'\n",
    "\n",
    "    def _format_arg(self, opt, spec, val):\n",
    "        \"\"\"Convert input to appropriate format for spm\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        from nipype.utils.filemanip import copyfiles\n",
    "\n",
    "        if opt in ['t1_files', 'flair_files']:\n",
    "            val2 = copyfiles(val, os.path.abspath(\".\"))\n",
    "            return np.array(val2, dtype=object)\n",
    "\n",
    "        return super(LGA, self)._format_arg(opt, spec, val)\n",
    "\n",
    "    def _list_outputs(self):\n",
    "        from nipype.utils.filemanip import fname_presuffix\n",
    "        outputs = self._outputs().get()\n",
    "        kappa = round(self.inputs.kappa, 2)\n",
    "\n",
    "        if str(kappa) == '1.0':\n",
    "            kappa = 1\n",
    "\n",
    "        outputs[\"lesion_probability_map\"] = [fname_presuffix(f,\n",
    "                                                prefix=\"ples_lga_{}_rm\".format(kappa),\n",
    "                                                newpath=os.path.abspath(\".\")) for f in self.inputs.flair_files]\n",
    "        outputs[\"bias_corrected_flair\"] = [fname_presuffix(f,\n",
    "                                                prefix=\"rm\",\n",
    "                                                newpath=os.path.abspath(\".\")) for f in self.inputs.flair_files]\n",
    "        outputs[\"mat_file\"] = [fname_presuffix(f,\n",
    "                                                prefix=\"LST_lga_rm\",\n",
    "                                                newpath=os.path.abspath(\".\"),\n",
    "                                                use_ext=False)+\".mat\"  for f in self.inputs.flair_files]\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def mapper(Nt1, t1):\n",
    "    return [t1[Nt1]]\n",
    "\n",
    "def mapper2(Nt2, t2):\n",
    "    return [t2[Nt2]]\n",
    "\n",
    "\n",
    "class LGAIterInputSpec(PBRBaseInputSpec):\n",
    "    t1_files = InputMultiPath(File(exists=True))\n",
    "    flair_files = InputMultiPath(File(exists=True))\n",
    "\n",
    "class LGAIterOutputSpec(TraitedSpec):\n",
    "    lesion_probability_mask_lga = OutputMultiPath(File(exists=True))\n",
    "    lesion_binary_ref = OutputMultiPath(File(exists=True))\n",
    "    lga_mat = OutputMultiPath(File(exists=True))\n",
    "    lga_txt = OutputMultiPath(File(exists=True))\n",
    "    lga_index = OutputMultiPath(File(exists=True))\n",
    "    lga_metrics = OutputMultiPath(traits.Dict())\n",
    "\n",
    "class LGAIter(PBRBaseInterface):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    input_spec = LGAIterInputSpec\n",
    "    output_spec = LGAIterOutputSpec\n",
    "    flag = \"lga\" #this is for pbr mse# -w interfacename\n",
    "    connections = [(\"align\", \"t1_files\", \"t1_files\"),\n",
    "                   (\"align\", \"flair_files\", \"flair_files\")]\n",
    "\n",
    "    def _run_interface_pbr(self, runtime):\n",
    "        from nipype.pipeline.engine import Node, Workflow, MapNode\n",
    "        from nipype.interfaces.io import DataSink\n",
    "        import nipype.algorithms.misc as misc\n",
    "        from nipype.interfaces.utility import IdentityInterface, Function\n",
    "        import numpy as np\n",
    "\n",
    "        wf = Workflow(name=\"lga_%s\"%self.inputs.mseID)\n",
    "        wf.base_dir = os.path.join(config[\"working_directory\"])\n",
    "\n",
    "        #get_ratio_workflow(config)\n",
    "        inputspec = Node(IdentityInterface([\"t1s\", \"flairs\"]), name=\"inputspec\")\n",
    "        #wf.get_node(\"inputspec\")\n",
    "        #inputspec.inputs.examID = self.inputs.mseID\n",
    "\n",
    "        map_func = Node(Function(input_names=[\"Nt1\", \"t1\"],\n",
    "                                 output_names=[\"t1\"],\n",
    "                                 function=mapper),\n",
    "                        name=\"stupid_mapper\")\n",
    "\n",
    "        map_func2 = Node(Function(input_names=[\"Nt2\", \"t2\"],\n",
    "                                 output_names=[\"t2\"],\n",
    "                                 function=mapper2),\n",
    "                        name=\"stupid_mapper2\")\n",
    "\n",
    "        map_func.inputs.t1 = self.inputs.t1_files\n",
    "        map_func2.inputs.t2 = self.inputs.flair_files\n",
    "\n",
    "        map_func.iterables = [('Nt1', range(len(self.inputs.t1_files)))]\n",
    "        map_func2.iterables = [('Nt2', range(len(self.inputs.flair_files)))]\n",
    "\n",
    "        wf.connect(map_func, \"t1\", inputspec, \"t1s\")\n",
    "        wf.connect(map_func2, \"t2\", inputspec, \"flairs\")\n",
    "\n",
    "        gunzip_t1 = MapNode(interface=misc.Gunzip(), name='gunzip_t1', iterfield=[\"in_file\"])\n",
    "        gunzip_flair = MapNode(interface=misc.Gunzip(), name='gunzip_flair', iterfield=[\"in_file\"])\n",
    "        wf.connect(inputspec, \"t1s\", gunzip_t1, \"in_file\")\n",
    "        wf.connect(inputspec, \"flairs\", gunzip_flair, \"in_file\")\n",
    "\n",
    "        lga = Node(LGA(), name=\"lga\")\n",
    "        init_thresh = np.linspace(0.05, 1.00, num=20)\n",
    "        lga.iterables = (\"kappa\", init_thresh)\n",
    "        # or lga.inputs.kappa = init_thresh (?)\n",
    "        wf.connect(gunzip_t1, \"out_file\", lga, \"t1_files\")\n",
    "        wf.connect(gunzip_flair, \"out_file\", lga, \"flair_files\")\n",
    "\n",
    "        sinker = Node(DataSink(), name=\"sinker\")\n",
    "        sinker.inputs.base_directory = config[\"output_directory\"]\n",
    "        sinker.inputs.container = self.inputs.mseID\n",
    "\n",
    "        import nipype.interfaces.fsl as fsl\n",
    "        cluster_lga = MapNode(fsl.Cluster(threshold=0.0001,\n",
    "                                          out_index_file = True,\n",
    "                                          #out_localmax_txt_file=True,\n",
    "                                          use_mm=True),\n",
    "                              name=\"cluster_lga\",\n",
    "                              iterfield=[\"in_file\"])\n",
    "        wf.connect(lga, \"lesion_probability_map\", cluster_lga, \"in_file\")\n",
    "        wf.connect(cluster_lga, \"index_file\", sinker, \"lst.lga.cluster\")\n",
    "\n",
    "        from nipype.interfaces.freesurfer import SegStats\n",
    "        segstats_lga = MapNode(SegStats(), name=\"segstats_lga\", iterfield=[\"segmentation_file\"])\n",
    "        wf.connect(cluster_lga, \"index_file\", segstats_lga, \"segmentation_file\")\n",
    "\n",
    "        wf.connect(segstats_lga, \"summary_file\", sinker, \"lst.lga.@summaryfile\")\n",
    "\n",
    "        def getsubs(t1,t2):\n",
    "            # Make substitutions\n",
    "            subs = [(\"_Nt1_%d/_Nt2_%d/_segstats_lga0/summary.stats\"%(i,j),\n",
    "                     \"{}/{}_summary.stats\".format(t1[i].split(\"/\")[-1].split(\".nii.gz\")[0],\n",
    "                                     t2[j].split(\"/\")[-1].split(\".nii.gz\")[0])) \\\n",
    "                     for i in range(len(t1)) for j in range(len(t2))]\n",
    "            subs += [(\"_Nt1_%d/_Nt2_%d\"%(i,j),t1[i].split(\"/\")[-1].split(\".nii.gz\")[0]) for i in range(len(t1)) for j in range(len(t2))]\n",
    "            subs += [(\"_Nt2_%d\"%i, \"\") for i in range(10)]\n",
    "            subs += [(\"_cluster_lga0\", \"\")]\n",
    "            return subs\n",
    "\n",
    "        subs = Node(Function(input_names=[\"t1\", \"t2\"],\n",
    "                             output_names=[\"subs\"],\n",
    "                             function=getsubs),\n",
    "                    name=\"subs\")\n",
    "        #sinker.inputs.substitutions = getsubs()\n",
    "        wf.connect(map_func, \"t1\", subs, \"t1\")\n",
    "        wf.connect(map_func2, \"t2\", subs, \"t2\")\n",
    "        wf.connect(subs, \"subs\", sinker, \"substitutions\")\n",
    "        wf.connect(lga, \"lesion_probability_map\", sinker, \"lst.lga.@map\")\n",
    "        wf.connect(lga, \"mat_file\", sinker, \"lst.lga.@mat\")\n",
    "\n",
    "        wf.config = {\"execution\": {\"crashdump_dir\": os.path.join(config[\"crash_directory\"],\n",
    "                                                                 self.inputs.mseID,\n",
    "                                                                 self.flag)}}\n",
    "        wf.run(plugin=self.inputs.plugin,\n",
    "               plugin_args=self.inputs.plugin_args)\n",
    "\n",
    "        return runtime\n",
    "\n",
    "\n",
    "    def _get_output_folder(self):\n",
    "        # output folder for status.json\n",
    "        return \"lst/lga\"\n",
    "\n",
    "    def get_metrics(self, f):\n",
    "        import numpy as np\n",
    "        if os.path.exists(f):\n",
    "            foo = np.genfromtxt(f)\n",
    "            print(\"foo.shape is\", foo.shape)\n",
    "            if len(foo.shape)==2:\n",
    "                data = foo[:,3][1:]\n",
    "                output = {\"number_of_lesions\": data.shape[0],\n",
    "                          \"total_lesion_volume\": np.sum(data),\n",
    "                          \"max_lesion_size\": np.max(data)}\n",
    "                return output\n",
    "            else:\n",
    "                return {\"number_of_lesions\": 0}\n",
    "        else:\n",
    "            raise FileNotFoundError\n",
    "\n",
    "\n",
    "    def _list_outputs(self):\n",
    "        # managing outputs for later calculate Dice Coefficient\n",
    "        import shutil  # copy2 also copy file metadata like file's creation and modification times\n",
    "        import numpy as np\n",
    "        from glob import glob\n",
    "\n",
    "        bin_map_dir = \"/data/henry1/tristan/LST/FLAIR-MPRAGE\" # TODO make a dictionary\n",
    "        bin_map = \"*-{0}*_bin_lesion_map.nii\".format(self.inputs.mseID)\n",
    "        src = ''.join(glob(os.path.join(bin_map_dir, self.inputs.mseID, bin_map)))\n",
    "        dstnames = glob(os.path.join(config[\"output_directory\"], self.inputs.mseID,\n",
    "                                     \"lst\", \"lga\", \"*\", \"_kappa_*\"))\n",
    "        for dst in dstnames:\n",
    "            shutil.copy(src, dst)\n",
    "\n",
    "        outputs = self._outputs().get()\n",
    "\n",
    "        outputs[\"lesion_probability_mask_lga\"] = sorted(glob(os.path.join(config[\"output_directory\"],\n",
    "                                                   self.inputs.mseID, \"lst\",\"lga\",\"*\",\n",
    "                                                   \"_kappa_*\", \"ples_lga*.nii\")))\n",
    "        outputs[\"lesion_binary_ref\"] = sorted(glob(os.path.join(config[\"output_directory\"], self.inputs.mseID,\n",
    "                                                                \"lst\", \"lga\", \"*\", \"_kappa_*\", bin_map)))\n",
    "        outputs[\"lga_mat\"] = sorted(glob(os.path.join(config[\"output_directory\"],\n",
    "                                                   self.inputs.mseID, \"lst\",\"lga\",\"*\",\n",
    "                                                   \"_kappa_*\", \"*.mat\")))\n",
    "        outputs[\"lga_txt\"] = sorted(glob(os.path.join(config[\"output_directory\"],\n",
    "                                                   self.inputs.mseID, \"lst\",\"lga\",\"*\",\n",
    "                                                   \"_kappa_*\", \"_segstats_lga0\", \"*.stats\")))\n",
    "        outputs[\"lga_index\"] = sorted(glob(os.path.join(config[\"output_directory\"],\n",
    "                                                   self.inputs.mseID, \"lst\",\"lga\",\"*\",\n",
    "                                                   \"_kappa_*\", \"cluster\", \"*_index.nii.gz\")))\n",
    "        outputs[\"lga_metrics\"] = [self.get_metrics(f) for f in outputs[\"lga_txt\"]]\n",
    "\n",
    "        return outputs\n",
    "\n",
    "register_workflow(LGAIter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__PBR interface for running LGA with kappa of increment 0.05__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sf713420'\n",
    "\n",
    "import os\n",
    "from nipype.interfaces.base import isdefined\n",
    "from utils import doit_workflow, flatten\n",
    "import sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    iter = sys.argv[1]\n",
    "    mse_test = ['mse3727', 'mse4413', 'mse4482', 'mse4739', 'mse4754']\n",
    "    data_ref = [glob('/data/henry7/PBR/subjects/{0}/lst/lga/ms*/*/*_bin_lesion_map.nii'.format(mse)) for mse in mse_test]\n",
    "    data_ref = flatten(data_ref)\n",
    "    print(data_ref,'\\n', len(data_ref))\n",
    "\n",
    "    output_dir = '/data/henry1/tristan/LST/opt_thresh_results/test_subjects'\n",
    "    sk_dir = os.path.join(output_dir)\n",
    "    \n",
    "    if iter == '-iter':\n",
    "        thresh_array = np.linspace(0.05, 1.00, num=20)\n",
    "    elif isdefined(iter):\n",
    "        raise ValueError(\"The option for iterate binary threshold is -iter\")\n",
    "\n",
    "    dwf = doit_workflow(data_ref, thresh_array, sink_dir=sk_dir)\n",
    "    dwf.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This script takes the first positional argument as option for either iterating or not binary threshold for running the LGA DC calculation workflow. __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. New Metric\n",
    "\n",
    "The new metric take both voxel-based and lesion-based into account. The voxel-based DC (vDC) was already calculated in previous method. To calculate lesion-based DC (lDC), an algorithm to find centroids of lesions was implemented. The script below will get centroids of both:\n",
    "1) lst_edits (reference) and map it to lga lesion. By doing this, False Negative (FN) and True Positive (TP1) are calculated.\n",
    "2) LGA probability lesion mapping it back to lst_edits. By doing this, False Positive (FP) and True Positive (TP2) are calculated\n",
    "\n",
    "As is known that vDC is caculated based on the formula:\n",
    "<center>vDC = 2TP / (2TP + FP + FN)<center>\n",
    "\n",
    "And lDC is calculated:\n",
    "<center>lDC = (TP1 + TP2) / (TP1 + TP2 + FP + FN)<center>\n",
    "\n",
    "The new metric (nDC) takes sum of 0.5 weighting of vDC and 0.5 weighting of lDC:\n",
    "<center>nDC = 0.5 * vDC + 0.5 * lDC<center>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__author__ = 'sf713420'\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import label\n",
    "from nipype.utils.filemanip import save_json, load_json\n",
    "from copy import deepcopy\n",
    "import operator\n",
    "\n",
    "class GetCenterLesion():\n",
    "\n",
    "    def __init__(self, filename_list):\n",
    "        self.filename_list = filename_list\n",
    "        self.centroids = {}\n",
    "        self.lesion_size = []\n",
    "\n",
    "    def run_centroids(self):\n",
    "        for filename in self.filename_list:\n",
    "            if len(self.filename_list) == 1:\n",
    "                kappa = 'reference'\n",
    "            else:\n",
    "                kappa = os.path.split(filename)[1].split('_')[2]\n",
    "                if kappa == '1':\n",
    "                    kappa = kappa + '.0'\n",
    "\n",
    "            img = nib.load(filename)\n",
    "            data = img.get_data()\n",
    "\n",
    "            # print(img.header)\n",
    "            labeled_img, nlabels = label(data > 0)\n",
    "            self.lesion_size = np.bincount(labeled_img.ravel())\n",
    "            # print(self.lesion_size)\n",
    "            centroid = {}\n",
    "            centroid['present'] = {}\n",
    "            centroid['missing'] = {}\n",
    "            for idx in range(1, len(self.lesion_size)):\n",
    "                # idx is the index for labeled lesions\n",
    "                # centroid['present'][idx] = {}\n",
    "                l = (labeled_img == idx)\n",
    "                l_coord = np.nonzero(l)\n",
    "                # print(labeled_img.shape)\n",
    "                # print(l_coord)\n",
    "\n",
    "                x = x_sub = l_coord[0]\n",
    "                y = y_sub = l_coord[1]\n",
    "                z = z_sub = l_coord[2]\n",
    "                x_mean = np.mean(x)\n",
    "                y_mean = np.mean(y)\n",
    "                z_mean = np.mean(z)\n",
    "\n",
    "                x_round = int(round(x_mean))\n",
    "                y_round = int(round(y_mean))\n",
    "                z_round = int(round(z_mean))\n",
    "                # print(x_round, y_round, z_round)\n",
    "\n",
    "                for i in range(self.lesion_size[idx]):\n",
    "                    if x_round == x[i] and y_round == y[i] and z_round == z[i]:\n",
    "                        # print(\"Yes the centroid is in the shape\")\n",
    "                        # print(labeled_img[x_round, y_round, z_round])\n",
    "                        centroid['present'][idx] = [x_round, y_round, z_round]\n",
    "                        # centroid['present'][idx]['LesionSize'] = self.lesion_size[idx]\n",
    "                        break\n",
    "                    x_sub = np.delete(x_sub, 0)\n",
    "                    y_sub = np.delete(y_sub, 0)\n",
    "                    z_sub = np.delete(z_sub, 0)\n",
    "\n",
    "                if  len(x_sub) == 0 and len(y_sub) == 0 and len(z_sub) == 0:\n",
    "                    # centroid['missing'][idx] = [x_round, y_round, z_round]\n",
    "                    # assumption is that there must be voxels in z_round plane\n",
    "                    print(\"Oops, this centroid is not located in the lesion shape \\n\",\n",
    "                          \"The coordinate is (x, y, z): \", [x_round, y_round, z_round], '\\n',\n",
    "                          \"lesion size is: \", self.lesion_size[idx], \"\\n\",\n",
    "                          \"The lesion coordinate is: \", l_coord,\n",
    "                          \"\\n\",\n",
    "                          \"going to the 2nd level\")\n",
    "\n",
    "                    x_in_right = []\n",
    "                    y_in_right = []\n",
    "                    miss_coord_right = []\n",
    "                    x_in_left = []\n",
    "                    y_in_left = []\n",
    "                    miss_coord_left = []\n",
    "                    x_in_up = []\n",
    "                    y_in_up = []\n",
    "                    miss_coord_up = []\n",
    "                    x_in_down = []\n",
    "                    y_in_down = []\n",
    "                    miss_coord_down = []\n",
    "                    for i in range(self.lesion_size[idx]):\n",
    "                        if z[i] == z_round:\n",
    "                            if x[i] >= x_round:\n",
    "                                x_in_right.append(x[i])\n",
    "                                y_in_right.append(y[i])\n",
    "                                miss_coord_right.append([x[i], y[i], z[i]])\n",
    "                            if x[i] <= x_round:\n",
    "                                x_in_left.append(x[i])\n",
    "                                y_in_left.append(y[i])\n",
    "                                miss_coord_left.append((x[i], y[i], z[i]))\n",
    "                            if y[i] >= y_round:\n",
    "                                x_in_up.append(x[i])\n",
    "                                y_in_up.append(y[i])\n",
    "                                miss_coord_up.append([x[i], y[i], z[i]])\n",
    "                            if y[i] <= y_round:\n",
    "                                x_in_down.append(x[i])\n",
    "                                y_in_down.append(y[i])\n",
    "                                miss_coord_down.append([x[i], y[i], z[i]])\n",
    "\n",
    "                    # print(miss_coord)\n",
    "                    #coord_list = [len(miss_coord_right), len(miss_coord_up), len(miss_coord_left), len(miss_coord_down)]\n",
    "                    #max_index, max_value = max(enumerate(coord_list), key=operator.itemgetter(1))\n",
    "\n",
    "                    if len(miss_coord_right) >= len(miss_coord_left):\n",
    "                        new_miss_coord_lr = miss_coord_right\n",
    "                        x_in_new_lr = x_in_right\n",
    "                        y_in_new_lr = y_in_right\n",
    "                    else:\n",
    "                        new_miss_coord_lr = miss_coord_left\n",
    "                        x_in_new_lr = x_in_left\n",
    "                        y_in_new_lr = y_in_left\n",
    "\n",
    "                    if len(miss_coord_up) >= len(miss_coord_down):\n",
    "                        new_miss_coord_ud = miss_coord_up\n",
    "                        x_in_new_ud = x_in_up\n",
    "                        y_in_new_ud = y_in_up\n",
    "                    else:\n",
    "                        new_miss_coord_ud = miss_coord_down\n",
    "                        x_in_new_ud = x_in_down\n",
    "                        y_in_new_ud = y_in_down\n",
    "\n",
    "                    if len(new_miss_coord_lr) >= len(new_miss_coord_ud):\n",
    "                        x_in_new = x_in_new_lr\n",
    "                        y_in_new = y_in_new_lr\n",
    "                        new_miss_coord = new_miss_coord_lr\n",
    "                    else:\n",
    "                        x_in_new = x_in_new_ud\n",
    "                        y_in_new = y_in_new_ud\n",
    "                        new_miss_coord = new_miss_coord_ud\n",
    "\n",
    "                    x_round2 = int(round(np.mean(x_in_new)))\n",
    "                    y_round2 = int(round(np.mean(y_in_new)))\n",
    "                    z_round2 = z_round\n",
    "                    x_sub2 = x_in_new\n",
    "                    y_sub2 = y_in_new\n",
    "                    z_sub2 = [z_round2] * len(new_miss_coord)\n",
    "\n",
    "                    for i in range(len(new_miss_coord)):\n",
    "                        if x_round2 == x_in_new[i] and y_round2 == y_in_new[i]:\n",
    "                            centroid['present'][idx] = [x_round2, y_round2, z_round2]\n",
    "                            # centroid['present'][idx]['LesionSize'] = self.lesion_size[idx]\n",
    "                            break\n",
    "                        x_sub2 = np.delete(x_sub2, 0)\n",
    "                        y_sub2 = np.delete(y_sub2, 0)\n",
    "                        z_sub2 = np.delete(z_sub2, 0)\n",
    "\n",
    "                    if len(x_sub2) == 0 and len(y_sub2) == 0 and len(z_sub2) == 0:\n",
    "                        print(\"Oops, this centroid is still not in the lesion shape \\n\",\n",
    "                              \"The coordinate is (x, y, z): \", [x_round2, y_round2, z_round2], '\\n',\n",
    "                              \"lesion size is: \", len(new_miss_coord), \"\\n\",\n",
    "                              \"The lesion coordinate is: \", new_miss_coord,\n",
    "                              \"\\n\",\n",
    "                              \"going to the 3rd level\")\n",
    "\n",
    "                        # Assumption is that there must be voxels in y_round2 axis\n",
    "                        x_in_right2D = []\n",
    "                        miss_coord_right2D = []\n",
    "                        x_in_left2D = []\n",
    "                        miss_coord_left2D = []\n",
    "                        y_in_up2D = []\n",
    "                        miss_coord_up2D = []\n",
    "                        y_in_down2D = []\n",
    "                        miss_coord_down2D = []\n",
    "\n",
    "                        for i in range(len(new_miss_coord)):\n",
    "                            if y_in_new[i] == y_round2:\n",
    "                                if x_in_new[i] >= x_round2:\n",
    "                                    x_in_right2D.append(x_in_new[i])\n",
    "                                    miss_coord_right2D.append([x_in_new[i], y_in_new[i]])\n",
    "                                if x_in_new[i] <= x_round2:\n",
    "                                    x_in_left2D.append(x_in_new[i])\n",
    "                                    miss_coord_left2D.append([x_in_new[i], y_in_new[i]])\n",
    "                            if x_in_new[i] == x_round2:\n",
    "                                if y_in_new[i] >= y_round2:\n",
    "                                    y_in_up2D.append(y_in_new[i])\n",
    "                                    miss_coord_up2D.append([x_in_new[i], y_in_new[i]])\n",
    "                                if y_in_new[i] <= y_round2:\n",
    "                                    y_in_down2D.append(y_in_new[i])\n",
    "                                    miss_coord_down2D.append([x_in_new[i], y_in_new[i]])\n",
    "\n",
    "                        if len(miss_coord_right2D) >= len(miss_coord_left2D):\n",
    "                            x_in_new2D = x_in_right2D\n",
    "                            new_miss_coord2D_lr = miss_coord_right2D\n",
    "                        else:\n",
    "                            x_in_new2D = x_in_left2D\n",
    "                            new_miss_coord2D_lr = miss_coord_left2D\n",
    "\n",
    "                        if len(miss_coord_up2D) >= len(miss_coord_down2D):\n",
    "                            y_in_new2D = y_in_up2D\n",
    "                            new_miss_coord2D_ud = miss_coord_up2D\n",
    "                        else:\n",
    "                            y_in_new2D = y_in_down2D\n",
    "                            new_miss_coord2D_ud = miss_coord_down2D\n",
    "\n",
    "                        if len(x_in_new2D) >= len(y_in_new2D):\n",
    "                            x_round3 = int(round(np.mean(x_in_new2D)))\n",
    "                            y_round3 = y_round2\n",
    "                            z_round3 = z_round2\n",
    "                            new_miss_coord2D = new_miss_coord2D_lr\n",
    "                            x_sub3 = x_in_new2D\n",
    "                            y_sub3 = [y_round3] * len(new_miss_coord2D)\n",
    "                            z_sub3 = [z_round3] * len(new_miss_coord2D)\n",
    "\n",
    "                            for i in range(len(new_miss_coord2D)):\n",
    "                                if x_in_new2D[i] == x_round3:\n",
    "                                    centroid['present'][idx] = [x_round3, y_round3, z_round3]\n",
    "                                    print(\"Congrats! The centroid is found in the 3rd level\")\n",
    "                                    # centroid['present'][idx]['LesionSize'] = self.lesion_size[idx]\n",
    "                                    break\n",
    "                                x_sub3 = np.delete(x_sub3, 0)\n",
    "                                y_sub3 = np.delete(y_sub3, 0)\n",
    "                                z_sub3 = np.delete(z_sub3, 0)\n",
    "                        else:\n",
    "                            x_round3 = x_round2\n",
    "                            y_round3 = int(round(np.mean(y_in_new2D)))\n",
    "                            z_round3 = z_round2\n",
    "                            new_miss_coord2D = new_miss_coord2D_ud\n",
    "                            x_sub3 = [x_round3] * len(new_miss_coord2D)\n",
    "                            y_sub3 = y_in_new2D\n",
    "                            z_sub3 = [z_round3] * len(new_miss_coord2D)\n",
    "\n",
    "                            for i in range(len(new_miss_coord2D)):\n",
    "                                if y_in_new2D[i] == y_round3:\n",
    "                                    centroid['present'][idx] = [x_round3, y_round3, z_round3]\n",
    "                                    print(\"Congrats! The centroid is found in the 3rd level\")\n",
    "                                    # centroid['present'][idx]['LesionSize'] = self.lesion_size[idx]\n",
    "                                    break\n",
    "                                x_sub3 = np.delete(x_sub3, 0)\n",
    "                                y_sub3 = np.delete(y_sub3, 0)\n",
    "                                z_sub3 = np.delete(z_sub3, 0)\n",
    "\n",
    "                        if len(x_sub3) == 0 and len(y_sub3) == 0 and len(z_sub3) == 0:\n",
    "                            # centroid['missing'][idx] = {}\n",
    "                            centroid['missing'][idx] = {'Level1': [x_round, y_round, z_round],\n",
    "                                                               'Level2': [x_round2, y_round2, z_round2],\n",
    "                                                               'Level3': [x_round3, y_round3, z_round3]}\n",
    "                            # centroid['missing'][idx]['LesionSize'] = self.lesion_size[idx]\n",
    "                            print(\"Oops, this algorithm did not work. \\n\",\n",
    "                                  \"The coordinate is (x, y, z): \", [x_round3, y_round3, z_round3], '\\n',\n",
    "                                  \"lesion size is: \", self.len(new_miss_coord2D), \"\\n\",\n",
    "                                  \"The lesion coordinate is: \", new_miss_coord2D,\n",
    "                                  \"\\n\",\n",
    "                                  \"All three levels of centroids are stored in the json file.\")\n",
    "\n",
    "            if len(centroid['present']) == idx:\n",
    "                print(\"Great, all the centroids are in the lesion shape! The kappa is: \", kappa)\n",
    "            elif not centroid['present'] and not centroid['missing']:\n",
    "                print(\"There was no lesion found in this scenario. The kappa is: \", kappa)\n",
    "            else:\n",
    "                raise ValueError(\":( Please try other algorithm to get a valid point of the lesion shape\")\n",
    "            if kappa == 'reference':\n",
    "                kappa_name = kappa\n",
    "            else:\n",
    "                kappa_name = '_kappa_' + kappa\n",
    "\n",
    "            centroid_temp = deepcopy(centroid)\n",
    "            self.centroids[kappa_name] = centroid_temp\n",
    "\n",
    "        return self.centroids\n",
    "\n",
    "    def make_lga_json(self, centroids = None):\n",
    "        if centroids is None:\n",
    "            centroids = self.run_centroids()\n",
    "\n",
    "        outdir = os.path.split(os.path.split(self.filename_list[0])[0])[0]\n",
    "        save_json(os.path.join(outdir, \"centroid_lga.json\"), centroids)\n",
    "        print(\"The json file is generated in the directory: \", outdir)\n",
    "\n",
    "def load_data(lstpath):\n",
    "    img = nib.load(lstpath)\n",
    "    data = img.get_data()\n",
    "    return data\n",
    "\n",
    "def cal_FP(f, lst_edit):\n",
    "    kappa_array = np.linspace(0.05, 1.0, 20)\n",
    "    if len(lst_edit) > 1:\n",
    "        raise ValueError(\"More than one lst_edits files, please check\")\n",
    "    elif len(lst_edit) == 0:\n",
    "        raise ValueError(\"No lst_edits file is found, please check your folder\")\n",
    "    elif len(lst_edit) == 1:\n",
    "        lst_edit = ''.join(lst_edit)\n",
    "    print(\"The lst_edit is in the directory: \", lst_edit)\n",
    "    lst_data = load_data(lst_edit)\n",
    "    # print(np.max(lst_data), np.min(lst_data))\n",
    "    Lesion = GetCenterLesion(f)\n",
    "    lesions = Lesion.run_centroids()\n",
    "    for num in kappa_array:\n",
    "        num_str = str(num)\n",
    "        kappa_name = '_kappa_' + num_str\n",
    "        if len(lesions[kappa_name]['missing']) != 0:\n",
    "            raise ValueError(\"There is some centroid missed from the algorithm. \"\n",
    "                             \"Please make sure all the center points are found in the lesion.\")\n",
    "        else:\n",
    "            print(\"Awesome, all the centroids are present in the lesion.\")\n",
    "\n",
    "        lesion_items = sorted(lesions[kappa_name]['present'].items())\n",
    "        FP = 0\n",
    "        TP = 0\n",
    "        lesions[kappa_name]['FalsePositives'] = []\n",
    "        lesions[kappa_name]['TruePositives_to_ref'] = []\n",
    "        for i, coord in lesion_items:\n",
    "            if lst_data[coord[0], coord[1], coord[2]] == 0:\n",
    "            # if lst_data[coord['xyz'][0], coord['xyz'][1], coord['xyz'][2]] == 0:\n",
    "                lesions[kappa_name]['FalsePositives'].append(i)\n",
    "                FP += 1\n",
    "            else:\n",
    "                lesions[kappa_name]['TruePositives_to_ref'].append(i)\n",
    "                TP += 1\n",
    "\n",
    "        lesions[kappa_name]['NumOfFP'] = FP\n",
    "        lesions[kappa_name]['NumOfTP_to_ref'] = TP\n",
    "    Lesion.make_lga_json(lesions)\n",
    "    return [FP, TP]\n",
    "\n",
    "def cal_FN(f, lst_edit):\n",
    "    kappa_array = np.linspace(0.05, 1.0, 20)\n",
    "    if len(lst_edit) > 1:\n",
    "        raise ValueError(\"More than one lst_edits files, please check\")\n",
    "    elif len(lst_edit) == 0:\n",
    "        raise ValueError(\"No lst_edits file is found, please check your folder\")\n",
    "\n",
    "    print(\"The lst_edit is in the directory: \", lst_edit)\n",
    "    lga_data = [load_data(f_kappa) for f_kappa in f]\n",
    "    print(\"There are {} elements in lga data\".format(len(lga_data)))\n",
    "    # print(\"Lga 0 is: \", lga_data[0])\n",
    "    print(\"Check if it's 1 for mse3727: \", lga_data[0][52,102,47], lga_data[0][52,103,47])\n",
    "\n",
    "    Lesion = GetCenterLesion(lst_edit)\n",
    "    lesions = Lesion.run_centroids()\n",
    "    ref_lesion = 'reference'\n",
    "    if len(lesions[ref_lesion]['missing']) != 0:\n",
    "        raise ValueError(\"There is some centroid missed from the algorithm. \"\n",
    "                             \"Please make sure all the center points are found in the lesion.\")\n",
    "    else:\n",
    "        print(\"Awesome, all the centroids are present in the lesion.\")\n",
    "\n",
    "    ref_lesion_to_kappas = {}\n",
    "\n",
    "    for k, num in enumerate(kappa_array):\n",
    "        num_str = str(num)\n",
    "        kappa_name = '_kappa_' + num_str\n",
    "        lesion_items = sorted(lesions[ref_lesion]['present'].items())\n",
    "        print(\"Lesion items are: \", lesion_items)\n",
    "        FN = 0\n",
    "        TP = 0\n",
    "        lesions[ref_lesion]['FalseNegatives'] = []\n",
    "        lesions[ref_lesion]['TruePositives_to_lga'] = []\n",
    "        for i, coord in lesion_items:\n",
    "            print(i, coord)\n",
    "            if lga_data[k][coord[0], coord[1], coord[2]] == 0:\n",
    "            # if lga_data[coord['xyz'][0], coord['xyz'][1], coord['xyz'][2]] == 0:\n",
    "                lesions[ref_lesion]['FalseNegatives'].append(i)\n",
    "                FN += 1\n",
    "                print(\"Oops, FN plus 1! FN is: \", FN)\n",
    "            else:\n",
    "                lesions[ref_lesion]['TruePositives_to_lga'].append(i)\n",
    "                TP += 1\n",
    "                print(\"Yay! TP plus 1! TP is: \", TP)\n",
    "\n",
    "        print(\"FN, TP and kappa are: \", FN, TP, kappa_name)\n",
    "        lesions[ref_lesion]['NumOfFN'] = FN\n",
    "        lesions[ref_lesion]['NumOfTP_to_lga'] = TP\n",
    "        print(\"After adding FN and TP, the lesion dictionary is: \", lesions)\n",
    "        lesions_temp = deepcopy(lesions)\n",
    "        ref_lesion_to_kappas[kappa_name] = lesions_temp\n",
    "        print(\"After making a bigger dict to ref_lesion_to_kappas: \", ref_lesion_to_kappas)\n",
    "\n",
    "    print(\"After all kappas, the ref_lesion_to_kappas looks: \", ref_lesion_to_kappas)\n",
    "    outdir = os.path.split(os.path.split(f[0])[0])[0]\n",
    "    save_json(os.path.join(outdir, \"centroid_lst_edit.json\"), ref_lesion_to_kappas)\n",
    "    print(\"The json file is generated in the directory: \", outdir)\n",
    "    return [FN, TP]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from glob import glob\n",
    "    test_mse = ['mse3727', 'mse4413', 'mse4482', 'mse4739', 'mse4754']\n",
    "    FPTP = {}\n",
    "    FNTP = {}\n",
    "    for mse in test_mse:\n",
    "        f = sorted(glob('/data/henry7/PBR/subjects/{0}/lst/lga/ms*/_kappa_*/ples_lga_*_rmms*.nii'.format(mse)))\n",
    "        lst_edit = glob('/data/henry7/PBR/subjects/{0}/mindcontrol/ms*/lst/lst_edits/no_FP_filled_FN*'.format(mse))\n",
    "        FPTP[mse] = cal_FP(f, lst_edit)\n",
    "        # for cal_FN debugging\n",
    "        FNTP[mse] = cal_FN(f, lst_edit)\n",
    "        # A lot 2nd level centroid in mse4482 and mse4754\n",
    "        # No lesion for mse3327 when kappa is 1.0\n",
    "        # No lesion for mse4439 when kappa is or greater than 0.8\n",
    "\n",
    "    print(FPTP, FNTP)\n",
    "    \"\"\"\n",
    "        outdir = os.path.split(os.path.split(f[0])[0])[0]\n",
    "        save_json(os.path.join(outdir, \"centroid_lga.json\"), lesions)\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Results\n",
    "\n",
    "### A. Table of Binary Threshold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Dice_Coefficient</th>\n",
       "      <th>SE</th>\n",
       "      <th>SP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.475833</td>\n",
       "      <td>0.358878</td>\n",
       "      <td>0.999789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.468314</td>\n",
       "      <td>0.349578</td>\n",
       "      <td>0.999817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.463385</td>\n",
       "      <td>0.343767</td>\n",
       "      <td>0.999832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.20</td>\n",
       "      <td>0.459555</td>\n",
       "      <td>0.339401</td>\n",
       "      <td>0.999842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.456596</td>\n",
       "      <td>0.336025</td>\n",
       "      <td>0.999849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.30</td>\n",
       "      <td>0.454107</td>\n",
       "      <td>0.333195</td>\n",
       "      <td>0.999855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.451778</td>\n",
       "      <td>0.330667</td>\n",
       "      <td>0.999859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.40</td>\n",
       "      <td>0.449665</td>\n",
       "      <td>0.328433</td>\n",
       "      <td>0.999862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.447638</td>\n",
       "      <td>0.326332</td>\n",
       "      <td>0.999865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.50</td>\n",
       "      <td>0.446124</td>\n",
       "      <td>0.324714</td>\n",
       "      <td>0.999868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.444625</td>\n",
       "      <td>0.323119</td>\n",
       "      <td>0.999870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.60</td>\n",
       "      <td>0.443311</td>\n",
       "      <td>0.321718</td>\n",
       "      <td>0.999872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.65</td>\n",
       "      <td>0.441971</td>\n",
       "      <td>0.320329</td>\n",
       "      <td>0.999874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.440737</td>\n",
       "      <td>0.319028</td>\n",
       "      <td>0.999876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.75</td>\n",
       "      <td>0.439719</td>\n",
       "      <td>0.317954</td>\n",
       "      <td>0.999878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.80</td>\n",
       "      <td>0.438657</td>\n",
       "      <td>0.316840</td>\n",
       "      <td>0.999879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.85</td>\n",
       "      <td>0.437700</td>\n",
       "      <td>0.315826</td>\n",
       "      <td>0.999880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.90</td>\n",
       "      <td>0.436781</td>\n",
       "      <td>0.314869</td>\n",
       "      <td>0.999882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.95</td>\n",
       "      <td>0.435900</td>\n",
       "      <td>0.313967</td>\n",
       "      <td>0.999882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.435084</td>\n",
       "      <td>0.313113</td>\n",
       "      <td>0.999884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Threshold  Dice_Coefficient        SE        SP\n",
       "0        0.05          0.475833  0.358878  0.999789\n",
       "1        0.10          0.468314  0.349578  0.999817\n",
       "2        0.15          0.463385  0.343767  0.999832\n",
       "3        0.20          0.459555  0.339401  0.999842\n",
       "4        0.25          0.456596  0.336025  0.999849\n",
       "5        0.30          0.454107  0.333195  0.999855\n",
       "6        0.35          0.451778  0.330667  0.999859\n",
       "7        0.40          0.449665  0.328433  0.999862\n",
       "8        0.45          0.447638  0.326332  0.999865\n",
       "9        0.50          0.446124  0.324714  0.999868\n",
       "10       0.55          0.444625  0.323119  0.999870\n",
       "11       0.60          0.443311  0.321718  0.999872\n",
       "12       0.65          0.441971  0.320329  0.999874\n",
       "13       0.70          0.440737  0.319028  0.999876\n",
       "14       0.75          0.439719  0.317954  0.999878\n",
       "15       0.80          0.438657  0.316840  0.999879\n",
       "16       0.85          0.437700  0.315826  0.999880\n",
       "17       0.90          0.436781  0.314869  0.999882\n",
       "18       0.95          0.435900  0.313967  0.999882\n",
       "19       1.00          0.435084  0.313113  0.999884"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from IPython.display import display, HTML\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv(\"/data/henry1/tristan/LST/opt_thresh_results/FLAIR-MPRAGE/averages.csv\")\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Table 1. Averaged Dice Coefficient, SE and SP for all FLAIR-MPRAGE mses with 0.05 increment of binary threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is shown in the table, DC decreases with binary threshold increasing. This indicate that the binary threshold has monotonous effect in determining DC. In this case, we will use binary threshold = 0.3 for the later determination of optimal initial threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Table of Initial Threshold Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         FLAIR  kappa        DC        SE       SP\n",
      "0   rmms1624-mse3727-077-FLAIR   0.05  0.551630  0.481400  0.99923\n",
      "1   rmms1624-mse3727-077-FLAIR   0.10  0.533100  0.379960  0.99987\n",
      "2   rmms1624-mse3727-077-FLAIR   0.15  0.462580  0.304510  0.99996\n",
      "3   rmms1624-mse3727-077-FLAIR   0.20  0.395480  0.247350  0.99999\n",
      "4   rmms1624-mse3727-077-FLAIR   0.25  0.338160  0.203700  1.00000\n",
      "5   rmms1624-mse3727-077-FLAIR   0.30  0.312060  0.184990  1.00000\n",
      "6   rmms1624-mse3727-077-FLAIR   0.35  0.269330  0.155680  1.00000\n",
      "7   rmms1624-mse3727-077-FLAIR   0.40  0.248320  0.141760  1.00000\n",
      "8   rmms1624-mse3727-077-FLAIR   0.45  0.233200  0.131990  1.00000\n",
      "9   rmms1624-mse3727-077-FLAIR   0.50  0.206520  0.115150  1.00000\n",
      "10  rmms1624-mse3727-077-FLAIR   0.55  0.196780  0.109120  1.00000\n",
      "11  rmms1624-mse3727-077-FLAIR   0.60  0.192030  0.106210  1.00000\n",
      "12  rmms1624-mse3727-077-FLAIR   0.65  0.178690  0.098109  1.00000\n",
      "13  rmms1624-mse3727-077-FLAIR   0.70  0.164090  0.089379  1.00000\n",
      "14  rmms1624-mse3727-077-FLAIR   0.75  0.147830  0.079817  1.00000\n",
      "15  rmms1624-mse3727-077-FLAIR   0.80  0.109260  0.057784  1.00000\n",
      "16  rmms1624-mse3727-077-FLAIR   0.85  0.018942  0.009561  1.00000\n",
      "17  rmms1624-mse3727-077-FLAIR   0.90  0.009928  0.004989  1.00000\n",
      "18  rmms1624-mse3727-077-FLAIR   0.95  0.000831  0.000416  1.00000\n",
      "19  rmms1624-mse3727-077-FLAIR   1.00  0.000000  0.000000  1.00000\n",
      "20   rmms736-mse4413-010-FLAIR   0.05  0.518860  0.586340  0.99874\n",
      "21   rmms736-mse4413-010-FLAIR   0.10  0.564910  0.484560  0.99957\n",
      "22   rmms736-mse4413-010-FLAIR   0.15  0.523940  0.383210  0.99985\n",
      "23   rmms736-mse4413-010-FLAIR   0.20  0.481540  0.326230  0.99995\n",
      "24   rmms736-mse4413-010-FLAIR   0.25  0.418600  0.266200  0.99999\n",
      "25   rmms736-mse4413-010-FLAIR   0.30  0.384180  0.238800  0.99999\n",
      "26   rmms736-mse4413-010-FLAIR   0.35  0.344830  0.208790  1.00000\n",
      "27   rmms736-mse4413-010-FLAIR   0.40  0.290070  0.169640  1.00000\n",
      "28   rmms736-mse4413-010-FLAIR   0.45  0.245040  0.139630  1.00000\n",
      "29   rmms736-mse4413-010-FLAIR   0.50  0.239660  0.136150  1.00000\n",
      "30   rmms736-mse4413-010-FLAIR   0.55  0.210200  0.117440  1.00000\n",
      "31   rmms736-mse4413-010-FLAIR   0.60  0.177570  0.097434  1.00000\n",
      "32   rmms736-mse4413-010-FLAIR   0.65  0.164470  0.089604  1.00000\n",
      "33   rmms736-mse4413-010-FLAIR   0.70  0.145220  0.078295  1.00000\n",
      "34   rmms736-mse4413-010-FLAIR   0.75  0.114800  0.060896  1.00000\n",
      "35   rmms736-mse4413-010-FLAIR   0.80  0.091324  0.047847  1.00000\n",
      "36   rmms736-mse4413-010-FLAIR   0.85  0.061551  0.031753  1.00000\n",
      "37   rmms736-mse4413-010-FLAIR   0.90  0.046729  0.023923  1.00000\n",
      "38   rmms736-mse4413-010-FLAIR   0.95  0.045068  0.023054  1.00000\n",
      "39   rmms736-mse4413-010-FLAIR   1.00  0.037559  0.019139  1.00000\n",
      "40   rmms870-mse4482-010-FLAIR   0.05  0.745330  0.686420  0.99801\n",
      "41   rmms870-mse4482-010-FLAIR   0.10  0.726020  0.621900  0.99883\n",
      "42   rmms870-mse4482-010-FLAIR   0.15  0.689420  0.557390  0.99924\n",
      "43   rmms870-mse4482-010-FLAIR   0.20  0.646170  0.497680  0.99945\n",
      "44   rmms870-mse4482-010-FLAIR   0.25  0.609110  0.452330  0.99958\n",
      "45   rmms870-mse4482-010-FLAIR   0.30  0.565040  0.404460  0.99965\n",
      "46   rmms870-mse4482-010-FLAIR   0.35  0.521270  0.358480  0.99978\n",
      "47   rmms870-mse4482-010-FLAIR   0.40  0.433960  0.280810  0.99983\n",
      "48   rmms870-mse4482-010-FLAIR   0.45  0.387850  0.242840  0.99988\n",
      "49   rmms870-mse4482-010-FLAIR   0.50  0.346220  0.210980  0.99990\n",
      "50   rmms870-mse4482-010-FLAIR   0.55  0.287110  0.168430  0.99994\n",
      "51   rmms870-mse4482-010-FLAIR   0.60  0.237950  0.135430  0.99996\n",
      "52   rmms870-mse4482-010-FLAIR   0.65  0.166590  0.091049  0.99997\n",
      "53   rmms870-mse4482-010-FLAIR   0.70  0.134310  0.072119  0.99998\n",
      "54   rmms870-mse4482-010-FLAIR   0.75  0.095202  0.050043  0.99998\n",
      "55   rmms870-mse4482-010-FLAIR   0.80  0.073238  0.038033  0.99999\n",
      "56   rmms870-mse4482-010-FLAIR   0.85  0.059675  0.030769  0.99999\n",
      "57   rmms870-mse4482-010-FLAIR   0.90  0.046909  0.024021  1.00000\n",
      "58   rmms870-mse4482-010-FLAIR   0.95  0.041774  0.021333  1.00000\n",
      "59   rmms870-mse4482-010-FLAIR   1.00  0.040677  0.020761  1.00000\n",
      "60   rmms779-mse4739-011-FLAIR   0.05  0.230330  0.544060  0.99940\n",
      "61   rmms779-mse4739-011-FLAIR   0.10  0.311400  0.371650  0.99981\n",
      "62   rmms779-mse4739-011-FLAIR   0.15  0.147470  0.122610  0.99990\n",
      "63   rmms779-mse4739-011-FLAIR   0.20  0.086957  0.065134  0.99992\n",
      "64   rmms779-mse4739-011-FLAIR   0.25  0.034783  0.022989  0.99994\n",
      "65   rmms779-mse4739-011-FLAIR   0.30  0.012308  0.007663  0.99996\n",
      "66   rmms779-mse4739-011-FLAIR   0.35  0.000000  0.000000  0.99997\n",
      "67   rmms779-mse4739-011-FLAIR   0.40  0.000000  0.000000  0.99998\n",
      "68   rmms779-mse4739-011-FLAIR   0.45  0.000000  0.000000  0.99999\n",
      "69   rmms779-mse4739-011-FLAIR   0.50  0.000000  0.000000  0.99999\n",
      "70   rmms779-mse4739-011-FLAIR   0.55  0.000000  0.000000  0.99999\n",
      "71   rmms779-mse4739-011-FLAIR   0.60  0.000000  0.000000  0.99999\n",
      "72   rmms779-mse4739-011-FLAIR   0.65  0.000000  0.000000  1.00000\n",
      "73   rmms779-mse4739-011-FLAIR   0.70  0.000000  0.000000  1.00000\n",
      "74   rmms779-mse4739-011-FLAIR   0.75  0.000000  0.000000  1.00000\n",
      "75   rmms779-mse4739-011-FLAIR   0.80  0.000000  0.000000  1.00000\n",
      "76   rmms779-mse4739-011-FLAIR   0.85  0.000000  0.000000  1.00000\n",
      "77   rmms779-mse4739-011-FLAIR   0.90  0.000000  0.000000  1.00000\n",
      "78   rmms779-mse4739-011-FLAIR   0.95  0.000000  0.000000  1.00000\n",
      "79   rmms779-mse4739-011-FLAIR   1.00  0.000000  0.000000  1.00000\n",
      "80  rmms1206-mse4754-011-FLAIR   0.05  0.710770  0.791500  0.99693\n",
      "81  rmms1206-mse4754-011-FLAIR   0.10  0.747340  0.693460  0.99885\n",
      "82  rmms1206-mse4754-011-FLAIR   0.15  0.721760  0.615050  0.99937\n",
      "83  rmms1206-mse4754-011-FLAIR   0.20  0.677240  0.537130  0.99965\n",
      "84  rmms1206-mse4754-011-FLAIR   0.25  0.629310  0.473020  0.99979\n",
      "85  rmms1206-mse4754-011-FLAIR   0.30  0.581400  0.418780  0.99985\n",
      "86  rmms1206-mse4754-011-FLAIR   0.35  0.547880  0.383470  0.99988\n",
      "87  rmms1206-mse4754-011-FLAIR   0.40  0.504270  0.340570  0.99993\n",
      "88  rmms1206-mse4754-011-FLAIR   0.45  0.467080  0.306830  0.99995\n",
      "89  rmms1206-mse4754-011-FLAIR   0.50  0.420930  0.267780  0.99997\n",
      "90  rmms1206-mse4754-011-FLAIR   0.55  0.366620  0.225070  0.99998\n",
      "91  rmms1206-mse4754-011-FLAIR   0.60  0.317650  0.189070  0.99999\n",
      "92  rmms1206-mse4754-011-FLAIR   0.65  0.276760  0.160670  1.00000\n",
      "93  rmms1206-mse4754-011-FLAIR   0.70  0.246540  0.140650  1.00000\n",
      "94  rmms1206-mse4754-011-FLAIR   0.75  0.222260  0.125060  1.00000\n",
      "95  rmms1206-mse4754-011-FLAIR   0.80  0.201330  0.111940  1.00000\n",
      "96  rmms1206-mse4754-011-FLAIR   0.85  0.185900  0.102480  1.00000\n",
      "97  rmms1206-mse4754-011-FLAIR   0.90  0.172330  0.094289  1.00000\n",
      "98  rmms1206-mse4754-011-FLAIR   0.95  0.156880  0.085117  1.00000\n",
      "99  rmms1206-mse4754-011-FLAIR   1.00  0.149480  0.080777  1.00000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "df2 = pd.read_csv(\"/data/henry1/tristan/LST/opt_thresh_results/test_subjects/_bin_thresh_0.3/For_Jupyter_Display.csv\")\n",
    "print_full(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table 2. Initial threshold (kappa) with increment of 0.05 for 5 mse subjects. The DC, SE and SP are displayed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. New Metric (To be continued)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## IV. Discussion\n",
    "\n",
    "### A. \n",
    "The threshold in Table 1 is binary threshold for the computation of LGA binary lesion maps, The threshold can be set between 0 to 1, where the thresholding is more liberal when it's set close to 0 and more conservative when it's set close 1. \n",
    "We concluded from Table 1 the binary threshold has a monotonous effect in determining DC, it will not affect the accuracy of determination of kappa across different subjects as long as binary threshold is kept the same. In this case, binary threshold is set to 0.3 for future determination of optimal initial threshold.\n",
    "\n",
    "### B. \n",
    "The table in our results displays the outputs of LST’s provided tool for determining the optimal threshold for 5 test subjects iterated over 20 values of kappa (0.05 - 1.0).  Our iteration through various kappa values shows us that the Dice Coefficient is generally inversely proportional to the initial threshold. This implies that a lower kappa value will produce a more accurate lesion map. However, upon inspection of the lesion maps in comparison to manually created and edited lesion maps of the same test subject it appeared that this was not necessarily the case. It is true that we see the least False Negatives in the lesion maps created with the lowest kappa values. However these maps also create False Positives due to their extremely low threshold, and in some cases did not appear to have the most accurate lesion maps. This disparity between the lesion map with the highest Dice Coefficient and the most accurate lesion map calls for a new metric to be used in measuring LGA lesion maps’ accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
